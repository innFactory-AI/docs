---
title: KI Glossar
description: KI Glossar mit Erklärungen und Beschreibungen der gängigen Begriffe
---

Absolut! Hier ist das Glossar der 50 gängigen Begriffe und Abkürzungen im Bereich der Generativen KI, übersetzt ins Deutsche und in Tabellenform:

| Nr. | Begriff/Abkürzung            | Erklärung                                                                                                                                                                                                                                                                           |
| :-- | :--------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1   | **Aktivierungsfunktion**     | Eine mathematische Funktion, die auf die Ausgabe jedes Neurons in einem neuronalen Netz angewendet wird und bestimmt, ob und in welchem Ausmaß das Neuron aktiviert ("feuert") werden soll.                                                                                        |
| 2   | **Alignment**                | Der Prozess, sicherzustellen, dass die Ziele, Werte und Verhaltensweisen eines KI-Systems mit den menschlichen Absichten und ethischen Prinzipien übereinstimmen.                                                                                                                   |
| 3   | **Allgemeine Künstliche Intelligenz (AGI)** | Eine hypothetische Art von KI, die menschenähnliche kognitive Fähigkeiten in einem breiten Spektrum von Aufgaben besitzt, anstatt auf eine einzelne Aufgabe spezialisiert zu sein.                                                                                   |
| 4   | **Künstliche Intelligenz (KI)** | Das übergeordnete Feld der Informatik, das sich der Entwicklung von Maschinen widmet, die Aufgaben ausführen können, die typischerweise menschliche Intelligenz erfordern. Generative KI ist ein Teilgebiet der KI.                                                              |
| 5   | **Aufmerksamkeitsmechanismus (Attention Mechanism)** | Eine Komponente innerhalb neuronaler Netze, insbesondere Transformatoren, die es dem Modell ermöglicht, sich auf bestimmte Teile der Eingabesequenz zu konzentrieren, während es einen anderen Teil verarbeitet.                                                        |
| 6   | **Backpropagation**          | Ein Algorithmus, der in neuronalen Netzen verwendet wird, um die Gradienten der Verlustfunktion effizient in Bezug auf die Gewichte zu berechnen. Er ermöglicht es Modellen, aus ihren Fehlern zu lernen und Parameter während des Trainings anzupassen.                            |
| 7   | **Bias (Voreingenommenheit)** | Systematische Ungerechtigkeit oder Fehler in der Ausgabe oder dem Verhalten eines Modells, oft verursacht durch Voreingenommenheiten in den Trainingsdaten.                                                                                                                         |
| 8   | **Computer Vision (CV)**     | Ein KI-Feld, das Computern ermöglicht, visuelle Informationen aus Bildern und Videos zu "sehen", zu interpretieren und zu verstehen. Generative Modelle in CV können neue Bilder erstellen oder bestehende manipulieren.                                                              |
| 9   | **Kontextfenster (Context Window)** | Die maximale Menge an Text (Tokens), die ein Sprachmodell zu einem bestimmten Zeitpunkt bei der Generierung einer Antwort berücksichtigen oder "speichern" kann. Eingaben, die dieses Fenster überschreiten, werden oft abgeschnitten oder ignoriert.                           |
| 10  | **Deep Learning (DL)**       | Ein Teilbereich des maschinellen Lernens, der mehrschichtige neuronale Netze (tiefe neuronale Netze) verwendet, um komplexe Muster aus großen Datenmengen zu lernen. Die meisten fortschrittlichen generativen KI-Modelle basieren auf Deep Learning.                               |
| 11  | **Diffusionsmodell (Diffusion Model)** | Eine leistungsstarke Klasse generativer KI-Modelle, die lernen, Daten zu erzeugen, indem sie ein zufälliges Signal über viele Schritte hinweg schrittweise entrauschen. Sie haben herausragende Ergebnisse bei der Bild- und Audiogenerierung erzielt.                      |
| 12  | **Embeddings**               | Numerische Darstellungen (Vektoren) von Wörtern, Phrasen oder ganzen Dokumenten, die deren semantische Bedeutung und Beziehungen in einem hochdimensionalen Raum erfassen. Wörter mit ähnlicher Bedeutung liegen in diesem Raum näher beieinander.                                   |
| 13  | **Encoder-Decoder**          | Eine häufige Architektur neuronaler Netze, bei der ein Encoder die Eingabe in einen Kontextvektor verarbeitet und ein Decoder daraus eine Ausgabesequenz generiert. Transformatoren entwickelten sich aus diesem Konzept.                                                           |
| 14  | **Erklärbare KI (XAI)**      | Ein Bereich der KI-Forschung, der sich der Entwicklung von Methoden und Techniken widmet, um Entscheidungen und Vorhersagen von KI-Modellen für Menschen verständlich zu machen. Ziel ist es, die Transparenz und das Vertrauen in KI-Systeme zu erhöhen.                          |
| 15  | **Few-Shot Learning**        | Die Fähigkeit eines Modells, eine neue Aufgabe oder ein Konzept effektiv mit nur einer sehr geringen Anzahl von Beispielen zu erlernen. Dies steht im Gegensatz zum traditionellen maschinellen Lernen, das große Datensätze erfordert.                                                 |
| 16  | **Fine-tuning (Feinabstimmung)** | Der Prozess, ein vortrainiertes KI-Modell auf einem kleineren, spezifischen Datensatz weiter zu trainieren, um sein Wissen und seine Leistung an eine bestimmte Aufgabe oder Domäne anzupassen.                                                                                    |
| 17  | **Grundlagenmodell (Foundation Model)** | Ein sehr großes KI-Modell, typischerweise auf einem massiven Datensatz vortrainiert, das durch Feinabstimmung oder Prompt Engineering an eine Vielzahl nachgelagerter Aufgaben anpassbar ist. LLMs sind eine Art Grundlagenmodell.                                         |
| 18  | **Generative Adversarial Network (GAN)** | Eine Klasse generativer KI-Modelle, bestehend aus einem Generator und einem Diskriminator, die miteinander konkurrieren. Der Generator erstellt Daten, während der Diskriminator versucht, echte von generierten Daten zu unterscheiden, was zu immer realistischeren Ausgaben führt. |
| 19  | **Generative KI (GenAI)**    | Ein Zweig der KI, der sich auf die Erstellung neuer, origineller Inhalte wie Text, Bilder, Audio oder Code konzentriert, anstatt nur bestehende Daten zu analysieren. Sie nutzt komplexe Modelle, um Muster aus Trainingsdaten zu lernen und dann neuartige Ausgaben zu generieren. |
| 20  | **Gradientenabstieg (Gradient Descent)** | Ein Optimierungsalgorithmus, der verwendet wird, um die Kostenfunktion eines Modells zu minimieren, indem Parameter iterativ in Richtung des steilsten Abstiegs der Funktion angepasst werden. Er ist fundamental für das Training neuronaler Netze.                                  |
| 21  | **Halluzination**            | Ein Phänomen, bei dem ein generatives KI-Modell Ausgaben erzeugt, die faktisch falsch, unsinnig oder den Quelldaten untreu sind, obwohl sie plausibel klingen.                                                                                                                      |
| 22  | **Hyperparameter**           | Konfigurationsvariablen außerhalb des Modells, die vor Beginn des Trainingsprozesses festgelegt werden, wie z.B. Lernrate, Anzahl der Schichten oder Batch-Größe. Sie beeinflussen Training und Leistung eines Modells erheblich.                                                    |
| 23  | **Inferenz**                 | Der Prozess, ein trainiertes KI-Modell zu verwenden, um Vorhersagen zu treffen oder Ausgaben auf neuen, ungesehenen Daten zu generieren. Es ist die "Bereitstellungsphase", in der das Modell sein erlerntes Wissen anwendet.                                                           |
| 24  | **Großes Sprachmodell (LLM)** | Ein KI-Modelltyp, oft basierend auf der Transformer-Architektur, der auf riesigen Mengen von Textdaten trainiert wurde, um menschliche Sprache zu verstehen, zu generieren und zu verarbeiten. LLMs sind zu verschiedenen NLP-Aufgaben fähig.                                       |
| 25  | **Latenter Raum (Latent Space)** | Eine komprimierte, abstrakte und niedrigdimensionale Darstellung von Daten, die von einem neuronalen Netz gelernt wurde. Sie erfasst die wesentlichen Merkmale und Beziehungen innerhalb der Daten und ermöglicht eine effiziente Manipulation und Generierung.                       |
| 26  | **Maschinelles Lernen (ML)** | Ein Teilgebiet der KI, in dem Systeme aus Daten lernen, ohne explizit programmiert zu werden, Muster identifizieren und Vorhersagen treffen. Es ist die Grundlage für die meisten modernen KI-Anwendungen, einschließlich GenAI.                                                      |
| 27  | **Modellarchitektur**        | Das spezifische Design und die Anordnung von Schichten, Neuronen und Verbindungen innerhalb eines neuronalen Netzes. Sie definiert, wie das Modell Informationen verarbeitet und trägt wesentlich zu seinen Fähigkeiten bei.                                                           |
| 28  | **Multimodale KI**           | KI-Systeme, die Informationen aus mehreren Arten von Dateneingaben, wie Text, Bildern, Audio und Video, verarbeiten und integrieren können. Dies ermöglicht ein reichhaltigeres Verständnis und eine bessere Generierung von Inhalten.                                                |
| 29  | **Natürliche Sprachverarbeitung (NLP)** | Ein KI-Feld, das sich darauf konzentriert, Computern das Verstehen, Interpretieren und Generieren menschlicher Sprache zu ermöglichen. LLMs sind ein signifikanter Fortschritt innerhalb der NLP.                                                                               |
| 30  | **Neuronales Netz (NN)**     | Ein Rechenmodell, inspiriert von der Struktur und Funktion des menschlichen Gehirns, bestehend aus miteinander verbundenen Knoten (Neuronen), die in Schichten organisiert sind. Sie sind grundlegende Bausteine für Deep Learning und GenAI.                                         |
| 31  | **One-Shot Learning**        | Eine extreme Form des Few-Shot Learnings, bei der ein Modell ein neues Konzept oder eine Aufgabe aus nur einem einzigen Beispiel lernen kann. Es beruht typischerweise auf starkem Vorwissen oder ausgeklügelten Meta-Lerntechniken.                                                      |
| 32  | **Overfitting (Überanpassung)** | Ein Phänomen, bei dem ein Modell die Trainingsdaten zu gut lernt und Rauschen sowie spezifische Details erfasst, die nicht auf neue Daten übertragbar sind. Dies führt zu schlechter Leistung bei ungesehenen Beispielen.                                                               |
| 33  | **Parameter**                | Die internen Variablen eines neuronalen Netzes, die während des Trainingsprozesses gelernt werden und sein spezifisches Wissen und Verhalten definieren. Die Anzahl der Parameter gibt die Komplexität und Kapazität eines Modells an.                                                   |
| 34  | **Prompt**                   | Der spezifische Eingabetext oder die Anweisung, die einem generativen KI-Modell gegeben wird, um eine Antwort oder Generierung zu initiieren. Ein gut gestalteter Prompt ist entscheidend für die Erzielung relevanter und qualitativ hochwertiger Ergebnisse.                         |
| 35  | **Prompt Engineering**       | Die Kunst und Wissenschaft, effektive Eingaben (Prompts) für KI-Modelle, insbesondere LLMs, zu erstellen, um deren Verhalten zu steuern und gewünschte Ausgaben zu erhalten. Es beinhaltet die Strukturierung von Anfragen zur Optimierung der Modellantwort.                          |
| 36  | **Reinforcement Learning (RL)** | Eine Art des maschinellen Lernens, bei der ein Agent lernt, Entscheidungen zu treffen, indem er mit einer Umgebung interagiert und Belohnungen für wünschenswerte Aktionen sowie Strafen für unerwünschte erhält.                                                                     |
| 37  | **Reinforcement Learning from Human Feedback (RLHF)** | Eine Technik zur Abstimmung großer Sprachmodelle auf menschliche Präferenzen und Sicherheitsrichtlinien. Dabei wird ein Belohnungsmodell auf der Grundlage menschlicher Vergleiche von Modellausgaben trainiert, das dann das weitere Training des LLM leitet.                          |
| 38  | **Retrieval-Augmented Generation (RAG)** | Eine Technik, die die LLM-Generierung verbessert, indem sie relevante Informationen aus einer externen Wissensbasis abruft und in den Prompt des Modells integriert. Dies reduziert Halluzinationen und erdet Antworten in faktischen Daten.                                    |
| 39  | **Supervised Learning (Überwachtes Lernen)** | Eine Art des maschinellen Lernens, bei der das Modell aus einem gelabelten Datensatz lernt, d.h. jedes Eingabebeispiel hat eine entsprechende korrekte Ausgabe. Ziel des Modells ist es, eine Zuordnung von Eingaben zu Ausgaben zu lernen.                                |
| 40  | **Testdaten**                | Ein vollständig separater Datensatz, der weder während des Trainings noch der Validierung verwendet wird, um die endgültige Leistung und Generalisierungsfähigkeit eines trainierten Modells zu beurteilen. Er bietet eine unvoreingenommene Bewertung des Modells.                    |
| 41  | **Token**                    | Die grundlegende Texteinheit, die ein Sprachmodell verarbeitet, die ein Wort, ein Teil eines Wortes oder sogar ein einzelnes Zeichen sein kann. Modelle zerlegen Eingaben und Ausgaben in diese diskreten Einheiten.                                                                    |
| 42  | **Trainingsdaten**           | Der Datensatz, der verwendet wird, um einem KI-Modell das Erkennen von Mustern und das Treffen von Vorhersagen beizubringen. Die Qualität und Quantität dieser Daten sind entscheidend für die Leistung des Modells.                                                                 |
| 43  | **Transformer**              | Eine Architektur neuronaler Netze, die besonders effektiv für Sequenz-zu-Sequenz-Aufgaben wie Sprachübersetzung und -generierung ist, bekannt für ihren Selbstaufmerksamkeitsmechanismus. Sie verarbeitet alle Teile einer Eingabesequenz gleichzeitig und erfasst so lange Abhängigkeiten effizient. |
| 44  | **Underfitting (Unteranpassung)** | Ein Phänomen, bei dem ein Modell zu einfach ist, um die zugrunde liegenden Muster in den Trainingsdaten zu erfassen, was zu einer schlechten Leistung sowohl auf Trainings- als auch auf neuen Daten führt. Das Modell hat nicht genug gelernt.                                       |
| 45  | **Unsupervised Learning (Unüberwachtes Lernen)** | Eine Art des maschinellen Lernens, bei der das Modell aus ungelabelten Daten lernt und Muster, Strukturen oder Beziehungen ohne explizite Anleitung identifiziert. Clustering und Dimensionsreduktion sind gängige Anwendungen.                                                   |
| 46  | **Validierungsdaten**        | Eine Teilmenge der Trainingsdaten, die verwendet wird, um die Leistung eines Modells während des Trainings zu bewerten und seine Hyperparameter abzustimmen. Sie hilft, Überanpassung an den Trainingsdatensatz zu vermeiden.                                                              |
| 47  | **Variational Autoencoder (VAE)** | Eine Art von generativen neuronalen Netzen, die eine komprimierte, probabilistische Darstellung (latenter Raum) ihrer Eingabedaten lernen können. Sie können dann neue Datenbeispiele erzeugen, indem sie aus diesem latenten Raum sampeln.                                             |
| 48  | **Vektordatenbank**          | Eine spezialisierte Datenbank, die entwickelt wurde, um hochdimensionale Vektor-Embeddings effizient zu speichern, zu verwalten und abzurufen. Sie wird oft mit LLMs für Aufgaben wie semantische Suche oder Retrieval-Augmented Generation (RAG) verwendet.                            |
| 49  | **Visuelles Sprachmodell (VLM)** | Ein multimodales KI-Modell, das sowohl visuelle (Bilder, Video) als auch textuelle Informationen verstehen und verarbeiten kann. VLMs können Aufgaben wie Bildunterschriften, visuelle Fragebeantwortung und das Generieren von Bildern aus Text ausführen.                               |
| 50  | **Zero-Shot Learning**       | Die Fähigkeit eines Modells, eine Aufgabe auszuführen oder ein Konzept zu verstehen, für das es nie explizit trainiert wurde, indem es sich stattdessen auf sein allgemeines Wissen und seinen Kontext stützt. Es schließt aus ähnlichen gelernten Konzepten.                            |